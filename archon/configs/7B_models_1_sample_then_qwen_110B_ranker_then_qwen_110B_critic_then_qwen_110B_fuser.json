{
    "name": "7B_models_1_sample_then_qwen_110B_ranker_then_qwen_110B_critic_then_qwen_110B_fuser",
    "layers": [
        [
            {
                "type": "generator",
                "model": "snorkelai/Snorkel-Mistral-PairRM-DPO",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "mistralai/Mistral-7B-Instruct-v0.3",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "togethercomputer/StripedHyena-Nous-7B",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "Qwen/Qwen1.5-7B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "meta-llama/Llama-3-8b-chat-hf",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "google/gemma-2-9b-it",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "meta-llama/Meta-Llama-3-8B-Instruct-Turbo",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            }
        ],
        [
            {
                "type": "ranker",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "ranker_batch_size": 16,
                "model_type": "Together_API",
                "top_k": 3,
                "temperature": 0.7,
                "max_tokens": 2048,
                "use_critiques": false
            }
        ],
        [
            {
                "type": "critic",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 8192,
                "samples": 1
            }
        ],
        [
            {
                "type": "fuser",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 8192,
                "samples": 1
            }
        ]
    ]
}