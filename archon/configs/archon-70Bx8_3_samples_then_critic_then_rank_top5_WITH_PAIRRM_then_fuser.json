{
    "name": "archon-70Bx8_3_samples_then_critic_then_rank_top5_WITH_PAIRRM_then_fuser",
    "layers": [
        [
            {
                "type": "generator",
                "model": "Qwen/Qwen2-72B-Instruct",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "deepseek-ai/deepseek-llm-67b-chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "microsoft/WizardLM-2-8x22B",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "Qwen/Qwen1.5-72B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "meta-llama/Llama-3-70b-chat-hf",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "mistralai/Mixtral-8x22B-Instruct-v0.1",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            },
            {
                "type": "generator",
                "model": "databricks/dbrx-instruct",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 3
            }
        ],
        [
            {
                "type": "ranker",
                "model": "llm-blender/PairRM",
                "ranker_batch_size": 16,
                "model_type": "",
                "top_k": 5,
                "temperature": 0.7,
                "max_tokens": 2048,
                "use_critiques": false
            }
        ],
        [
            {
                "type": "critic",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 8192,
                "samples": 1
            }
        ],
        [
            {
                "type": "fuser",
                "model": "Qwen/Qwen1.5-110B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 8192,
                "samples": 1
            }
        ]
    ]
}