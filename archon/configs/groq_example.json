{
    "name": "Groq_example",
    "layers": [
        {
            "branches": 1,
            "models": [
                {
                    "type": "generator",
                    "model": "llama-3.1-70b-versatile",
                    "model_type": "Groq_API",
                    "top_k": 1,
                    "temperature": 0.7,
                    "max_tokens": 2048,
                    "samples": 1
                }
            ]
        }
    ]
}