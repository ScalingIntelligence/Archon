{
    "name": "archon-code_70Bx6_1_samples_then_unit_test_generation_with_gpt4_turbo_then_unit_test_evaluation_with_gpt4_turbo_then_pass_top_cand",
    "layers": [
        [
            {
                "type": "unit_test_generator",
                "model": "gpt-4-turbo",
                "model_type": "OpenAI_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1,
                "unit_test_cap": 5
            }
        ],
        [
            {
                "type": "generator",
                "model": "deepseek-ai/deepseek-coder-33b-instruct",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "Qwen/Qwen1.5-72B-Chat",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "microsoft/WizardLM-2-8x22B",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            },
            {
                "type": "generator",
                "model": "mistralai/Mixtral-8x22B-Instruct-v0.1",
                "model_type": "Together_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "samples": 1
            }
        ],
        [
            {
                "type": "unit_test_evaluator",
                "model": "gpt-4-turbo",
                "model_type": "OpenAI_API",
                "temperature": 0.7,
                "max_tokens": 2048,
                "remove_unit_tests_from_prompt": true,
                "samples": 1,
                "top_k": 1
            }
        ]
    ]
}